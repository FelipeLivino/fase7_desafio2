# FIAP - Faculdade de Inform√°tica e Administra√ß√£o Paulista

<p align="center">
<a href= "https://www.fiap.com.br/"><img src="assets/logo-fiap.png" alt="FIAP - Faculdade de Inform√°tica e Admnistra√ß√£o Paulista" border="0" width=40% height=40%></a>
</p>

<br>

# Detec√ß√£o de Animais com YOLOv5 e Streaming em Tempo Real com Raspberry Pi | Cap 1 - Despertar da rede neural

## Nome do grupo

Rumo ao NEXT!

## üë®‚Äçüéì Integrantes:

- Felipe Livino dos Santos (RM 563187)
- Daniel Veiga Rodrigues de Faria (RM 561410)
- Tomas Haru Sakugawa Becker (RM 564147)
- Daniel Tavares de Lima Freitas (RM 562625)
- Gabriel Konno Carrozza (RM 564468)

## üë©‚Äçüè´ Professores:

### Tutor(a)

- Leonardo Ruiz Orabona

### Coordenador(a)

- ANDR√â GODOI CHIOVATO

---
## üìú Descri√ß√£o
Este reposit√≥rio documenta a solu√ß√£o desenvolvida para a **FarmTech Solutions**, uma empresa fict√≠cia que enfrenta desafios na gest√£o de seus ativos biol√≥gicos. O monitoramento manual de animais em grandes propriedades √© um processo caro, demorado e sujeito a erros humanos. Como resposta a esse problema, este projeto apresenta uma **prova de conceito (PoC)** de um sistema de vis√£o computacional para **automatizar a detec√ß√£o e contagem de animais**.

Utilizando a robusta arquitetura **YOLOv5**, o objetivo foi treinar um modelo capaz de identificar e diferenciar duas classes de animais com caracter√≠sticas distintas: **aves (galinhas)** e **gado (vacas)**. A solu√ß√£o visa fornecer √† FarmTech uma base s√≥lida para desenvolver um sistema de monitoramento inteligente, capaz de gerar dados precisos em tempo real, otimizar a aloca√ß√£o de recursos e melhorar o bem-estar animal.

Al√©m do treinamento, o projeto avan√ßa para uma demonstra√ß√£o pr√°tica de ponta a ponta, implementando o modelo treinado em um sistema de *Edge AI*. Esta segunda fase utiliza um **Raspberry Pi** para capturar e transmitir v√≠deo em tempo real, validando a aplica√ß√£o da solu√ß√£o em um cen√°rio que simula o ambiente de produ√ß√£o.

## Servi√ßo de Alerta com AWS SNS

Para complementar o sistema de detec√ß√£o, foi implementado um servi√ßo de alerta utilizando o **AWS Simple Notification Service (SNS)**. Este servi√ßo envia notifica√ß√µes (e-mail ou SMS) para os funcion√°rios da fazenda sempre que um animal √© classificado como "doente" pelo modelo de vis√£o computacional.

### Como configurar

Para que o servi√ßo de alerta funcione, voc√™ precisa configurar suas credenciais da AWS e o ARN do t√≥pico SNS.

1.  **Credenciais da AWS:**
    O script utiliza a biblioteca `boto3` para se conectar √† AWS. As credenciais devem ser configuradas como vari√°veis de ambiente:
    ```bash
    export AWS_ACCESS_KEY_ID="SUA_CHAVE_DE_ACESSO"
    export AWS_SECRET_ACCESS_KEY="SUA_CHAVE_SECRETA"
    ```
    Substitua `"SUA_CHAVE_DE_ACESSO"` e `"SUA_CHAVE_SECRETA"` pelas suas credenciais da AWS.

2.  **ARN do T√≥pico SNS:**
    O ARN (Amazon Resource Name) do t√≥pico SNS para o qual os alertas ser√£o enviados tamb√©m deve ser configurado como uma vari√°vel de ambiente:
    ```bash
    export SNS_TOPIC_ARN="ARN_DO_SEU_TOPICO_SNS"
    ```
    Substitua `"ARN_DO_SEU_TOPICO_SNS"` pelo ARN do seu t√≥pico SNS. Voc√™ pode encontrar o ARN no console da AWS em **Simple Notification Service > Topics**.

### Como funciona

1.  O script `run_detection.py` processa as imagens da pasta `input_images`.
2.  Para cada imagem, o modelo YOLOv5 detecta os objetos presentes.
3.  O script verifica se algum dos objetos detectados tem um r√≥tulo que cont√©m a palavra "doente".
4.  Se um animal doente for detectado, o script formata uma mensagem de alerta e a envia para o t√≥pico SNS configurado.
5.  O SNS, por sua vez, envia a mensagem para todos os inscritos no t√≥pico (e-mails, n√∫meros de SMS, etc.).

### Exemplo de Alerta

Quando um animal doente √© detectado, uma mensagem semelhante √† seguinte √© enviada:

```
Assunto: Alerta de Sa√∫de Animal: VACA-001

Alerta de Sa√∫de Animal:

Animal ID (imagem): VACA-001
Status Detectado: Doente (vaca_doente)
A√ß√£o Sugerida: Veterin√°rio acionado para avalia√ß√£o e tratamento.
```

## Demonstra√ß√£o em V√≠deo
Assista a uma breve demonstra√ß√£o que abrange desde o processo de treinamento e a performance do modelo, at√© sua aplica√ß√£o pr√°tica em um projeto de detec√ß√£o de objetos em tempo real com Raspberry Pi e YOLOv5.

‚ñ∂Ô∏è **[Assista ao v√≠deo no YouTube](https://youtu.be/jYD97Fs7piI)**

O projeto est√° dividido em tr√™s partes:

1.  **Entrega 1:** Treinamento de um modelo YOLO customizado para detectar **vacas** e **galinhas**.
2.  **Entrega 2:** An√°lise comparativa entre o modelo customizado, o YOLO padr√£o e uma CNN treinada do zero.
3.  **"Ir Al√©m":** Implementa√ß√£o de um sistema de detec√ß√£o em tempo real com uma Raspberry Pi.

## O Dataset
Para treinar nosso modelo, foi constru√≠do um dataset customizado e focado:
- **Composi√ß√£o:** O conjunto de dados cont√©m **80 imagens**, divididas igualmente: **40 para a classe 'vaca'** e **40 para a classe 'galinha'**.
- **Rotula√ß√£o:** Cada imagem foi cuidadosamente anotada com a ferramenta **Make Sense AI**, onde caixas delimitadoras (bounding boxes) foram desenhadas em cada animal. As anota√ß√µes foram salvas em arquivos `.txt` no formato padr√£o do YOLO.
- **Organiza√ß√£o:** O dataset foi estruturado em diret√≥rios de `treino` e `validacao`, uma pr√°tica essencial para treinar o modelo com um conjunto de dados e testar sua performance em outro, evitando overfitting.

## Treinamento e Metodologia
O processo de treinamento foi conduzido no **Google Colab** para utilizar suas GPUs gratuitas, acelerando o processo. A metodologia adotada foi a experimenta√ß√£o para encontrar um equil√≠brio entre tempo de treinamento e performance do modelo:
1.  **Simula√ß√£o 1:** O modelo foi treinado por **30 √©pocas** para obter uma baseline de performance.
2.  **Simula√ß√£o 2:** O treinamento foi estendido para **60 √©pocas** para avaliar se um maior tempo de exposi√ß√£o aos dados resultaria em uma melhoria significativa na acur√°cia.

A compara√ß√£o detalhada entre essas simula√ß√µes, analisando m√©tricas como **precis√£o, recall e mAP (mean Average Precision)**, foi fundamental para as conclus√µes do projeto e est√° inteiramente documentada no notebook.
---

## üìÅ Estrutura de pastas
```
fase6_desafio1/
‚îú‚îÄ‚îÄ assets/                     # Cont√©m imagens usadas no README.
‚îú‚îÄ‚îÄ raspberry_pi_streamer/      # C√≥digo para o servidor de streaming de v√≠deo na Raspberry Pi.
‚îÇ   ‚îú‚îÄ‚îÄ app.py                  # Servidor Flask que captura e transmite o v√≠deo da c√¢mera.
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python para o Raspberry Pi.
‚îú‚îÄ‚îÄ treinamento/
‚îÇ   ‚îú‚îÄ‚îÄ FelipeLivinoDosSantos_rm563187_pbl_fase6.ipynb  # Notebook principal com as Entregas 1 e 2.
‚îÇ   ‚îú‚îÄ‚îÄ images/                 # Dataset com imagens de treino, valida√ß√£o e teste.
‚îÇ   ‚îú‚îÄ‚îÄ labels/                 # Arquivos de r√≥tulo (bounding boxes) para o dataset.
‚îÇ   ‚îú‚îÄ‚îÄ main.yaml               # Arquivo de configura√ß√£o do dataset para o YOLOv5.
‚îÇ   ‚îú‚îÄ‚îÄ yolov5/                 # C√≥pia local do reposit√≥rio do YOLOv5.
‚îÇ   ‚îî‚îÄ‚îÄ yolov5s_60.pt           # Melhor modelo treinado (60 √©pocas), usado na detec√ß√£o.
‚îÇ   ‚îî‚îÄ‚îÄ yolov5s_30.pt           # Modelo treinado (30 √©pocas).
‚îÇ   ‚îî‚îÄ‚îÄ yolov5s.pt              # Modelo Standard sem treinamento.
‚îÇ   ‚îî‚îÄ‚îÄ cnn/images              # Contem as imagens utilizadas para o treinamento da CNN
‚îú‚îÄ‚îÄ .gitignore                  # Arquivos e pastas ignorados pelo Git.
‚îú‚îÄ‚îÄ README.md                   # Este arquivo.
‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias Python para a m√°quina host (detec√ß√£o).
‚îî‚îÄ‚îÄ run_detection.py            # Script principal para executar a detec√ß√£o em tempo real no host.
```

## Tecnologias Utilizadas
- **Modelo de Detec√ß√£o:** YOLOv5
- **Linguagem de Programa√ß√£o:** Python
- **Ambiente de Desenvolvimento:** Google Colab e Jupyter Notebook
- **Bibliotecas Principais:** PyTorch, OpenCV, Pandas, Matplotlib, Seaborn
- **Ferramenta de Anota√ß√£o:** Make Sense AI
- **Raspberry Pi com m√≥dulo de c√¢mera:** Para a parte do "Ir Al√©m"

##  entregas do Desafio

### Entrega 1: Treinamento de Modelo YOLO Customizado

**Contexto:** Como desenvolvedor na FarmTech Solutions, o objetivo era demonstrar a um cliente o potencial de um sistema de vis√£o computacional.

**Objetivos e Metodologia:**

1.  **Cria√ß√£o do Dataset:** Foi montado um dataset com 80 imagens, sendo 40 para a classe **"vaca"** e 40 para a classe **"galinha"**. O conjunto foi dividido em 80% para treino (32 imagens de cada), 10% para valida√ß√£o (4 de cada) e 10% para teste (4 de cada).
2.  **Rotula√ß√£o:** As imagens de treinamento foram rotuladas usando a ferramenta online **Make Sense IA** para gerar os arquivos de *bounding box* necess√°rios para o treinamento do YOLO.
3.  **Treinamento:** O ambiente de treinamento foi configurado no Google Colab (e replicado no notebook local). Foram realizados dois ciclos de treinamento para avaliar o impacto do n√∫mero de √©pocas na performance do modelo:
    *   **Simula√ß√£o 1:** 30 √©pocas.
    *   **Simula√ß√£o 2:** 60 √©pocas.
4.  **An√°lise:** Os resultados de acur√°cia, perda (*loss*) e performance geral foram comparados entre as duas simula√ß√µes. As conclus√µes e os resultados visuais das detec√ß√µes nas imagens de teste est√£o detalhados no notebook `treinamento/FelipeLivinoDosSantos_rm563187_pbl_fase6.ipynb`.

### Entrega 2: An√°lise Comparativa de Modelos

**Contexto:** Com o modelo customizado pronto, o pr√≥ximo passo foi compar√°-lo com outras abordagens para entender suas vantagens e desvantagens.

**Objetivos e Metodologia:**

O notebook `treinamento/FelipeLivinoDosSantos_rm563187_pbl_fase6.ipynb` tamb√©m cobre esta entrega, onde foram implementadas e avaliadas tr√™s arquiteturas distintas:

1.  **YOLO Customizado (Entrega 1):** O modelo treinado com nosso pr√≥prio dataset.
2.  **YOLO Padr√£o:** A vers√£o tradicional do YOLO, para avaliar a detec√ß√£o sem o fine-tuning espec√≠fico.
3.  **CNN do Zero:** Uma Rede Neural Convolucional simple, treinada do zero, para classificar as imagens (sem detectar a localiza√ß√£o do objeto).

**Crit√©rios de Avalia√ß√£o:**
A compara√ß√£o entre os modelos foi baseada nos seguintes pontos:
-   **Facilidade de Uso/Integra√ß√£o:** Complexidade para configurar e usar cada modelo.
-   **Precis√£o do Modelo:** M√©tricas como mAP (mean Average Precision) para YOLO e acur√°cia para a CNN.
-   **Tempo de Treinamento:** Dura√ß√£o necess√°ria para treinar cada modelo.
-   **Tempo de Infer√™ncia:** Velocidade com que cada modelo processa uma nova imagem.

As conclus√µes e a avalia√ß√£o cr√≠tica comparando os pontos fortes e fracos de cada abordagem est√£o documentadas no final do notebook.

### "Ir Al√©m": Sistema de Detec√ß√£o em Tempo Real

Esta parte do projeto demonstra a aplica√ß√£o pr√°tica do modelo treinado. A arquitetura consiste em:

-   **Raspberry Pi Streamer:** Um servidor Flask (`raspberry_pi_streamer/app.py`) que utiliza a `picamera2` para transmitir o v√≠deo ao vivo pela rede.
-   **Detector Host:** Um script Python (`run_detection.py`) que recebe o stream de v√≠deo, processa os frames com o melhor modelo treinado (`treinamento/yolov5s_60.pt`) e exibe as detec√ß√µes em tempo real.


## An√°lise Completa no Notebook
Este README serve como uma introdu√ß√£o e um guia. **Toda a an√°lise t√©cnica, o c√≥digo-fonte, as visualiza√ß√µes de dados, os gr√°ficos de performance e a discuss√£o aprofundada dos resultados** est√£o consolidados no notebook Jupyter. Ele foi projetado para ser um documento autoexplicativo e reprodut√≠vel. O acesso pode ser feito pela sua IDE ap√≥s clonar o repositorio clique em `yolov5.ipynb`.

## Como Replicar o Treinamento üîß 
Siga este passo a passo detalhado para executar o notebook e replicar o processo de treinamento.

**Pr√©-requisitos:**
- Uma Conta Google para acessar o Google Drive e o Google Colab.

**Passos:**
1.  **Clone o Reposit√≥rio:**
    - Fa√ßa o download ou clone este reposit√≥rio para a sua m√°quina local.
2.  **Fa√ßa Upload para o Google Drive:**
    - No seu Google Drive, crie uma pasta principal para o projeto (ex: `FIAP_PBL6`).
    - Dentro dela, fa√ßa o upload de toda a pasta `treinamento/` contida neste reposit√≥rio.
3.  **Abra o Notebook no Colab:**
    - Navegue at√© a pasta no seu Google Drive, clique com o bot√£o direito no arquivo `yolov5.ipynb` e selecione "Abrir com > Google Colaboratory".
4.  **Conecte o Google Drive ao Colab:**
    - A primeira c√©lula de c√≥digo do notebook ir√° solicitar permiss√£o para montar seu Google Drive. Siga as instru√ß√µes para permitir o acesso. Isso √© crucial para que o notebook possa ler as imagens e labels.
5.  **Verifique os Caminhos:**
    - Certifique-se de que o caminho no arquivo `main.yaml` (e no pr√≥prio notebook) para as pastas de treino e valida√ß√£o corresponde √† estrutura que voc√™ criou no seu Google Drive.
6.  **Execute as C√©lulas em Sequ√™ncia:**
    - Execute cada c√©lula do notebook, desde a instala√ß√£o das depend√™ncias at√© o treinamento e a avalia√ß√£o final. Cada c√©lula √© comentada para explicar o que ela faz.

## Conclus√£o
Este projeto demonstrou com sucesso a viabilidade de utilizar o modelo YOLOv5 para a detec√ß√£o de animais em um contexto agr√≠cola. A compara√ß√£o entre os treinamentos de 30 e 60 √©pocas indicou que um maior tempo de treinamento levou a um modelo com maior acur√°cia, validando a metodologia. A solu√ß√£o desenvolvida serve como uma excelente prova de conceito para a FarmTech Solutions, abrindo caminho para a implementa√ß√£o de um sistema de monitoramento em larga escala que pode trazer efici√™ncia e precis√£o para a gest√£o do agroneg√≥cio.

**Treinamento do Modelo:** O treinamento do YOLOv5 no Google Colab foi um sucesso. O modelo treinado por 60 √©pocas apresentou um comportamento superior em acur√°cia e capacidade de generaliza√ß√£o, provando ser eficaz na distin√ß√£o entre as classes 'aves' e 'gado' com um dataset limitado. Isso valida a arquitetura YOLOv5 como uma excelente escolha para tarefas de detec√ß√£o customizadas.

# Projeto de Detec√ß√£o de Objetos com Raspberry Pi e YOLOv5

Este projeto implementa um sistema de detec√ß√£o de objetos em tempo real, utilizando um Raspberry Pi para captura e streaming de v√≠deo e um computador host para processamento e infer√™ncia com o modelo YOLOv5.

Este projeto foi desenvolvido como parte do desafio "Ir Al√©m" da disciplina de AI Computer Systems & Sensors.

## Arquitetura do Projeto

O sistema √© dividido em dois componentes principais que se comunicam via Wi-Fi:

```
+---------------------------+
|   Raspberry Pi 3          |
|---------------------------|
| - M√≥dulo de C√¢mera        |
| - Script Python (app.py)  |
|   - Captura com picamera2 |
|   - Servidor com Flask    |
+-------------+-------------+
              |
              | (Wi-Fi)
              | Stream de V√≠deo (MJPEG)
              v
+-------------+-------------+
|   Computador Host         |
|---------------------------|
| - Script Python (run_detection.py) |
|   - Recebe com OpenCV     |
|   - Inferencia com YOLOv5 |
|     (usando best.pt)      |
| - Exibe resultado         |
+---------------------------+
```

**Fluxo de Dados:**

1.  O script `app.py` no Raspberry Pi captura continuamente os quadros da c√¢mera.
2.  Cada quadro √© codificado como JPEG.
3.  Um servidor web Flask transmite esses quadros como um stream de v√≠deo no formato MJPEG (Motion JPEG).
4.  O script `run_detection.py` no computador host se conecta a esse stream.
5.  Ele l√™ quadro a quadro, realiza a detec√ß√£o de objetos usando o modelo YOLOv5 treinado (`best.pt`).
6.  O resultado, com as caixas delimitadoras (bounding boxes) desenhadas, √© exibido em uma janela na tela do host.

## Como Executar

Siga os passos abaixo para configurar e executar o projeto.

### 1. Configura√ß√£o do Raspberry Pi (Transmissor)

**Hardware:**
*   Raspberry Pi 3 com Raspberry Pi OS
*   M√≥dulo de C√¢mera V1.3 ou compat√≠vel, devidamente conectado e ativado (`sudo raspi-config`).

**Passos:**

1.  **Clone o reposit√≥rio** ou copie a pasta `raspberry_pi_streamer` para o seu Raspberry Pi.

2. **Crie o ambiente na pasta do seu projeto**
    python -m venv venv

    # Ative o ambiente (Windows)
    .\venv\Scripts\activate

    # Ative o ambiente (Linux/macOS)
    # source venv/bin/activate

3.  **Instale as depend√™ncias:**
    ```bash
    cd raspberry_pi_streamer
    pip install -r requirements.txt
    ```

4.  **Execute o servidor de streaming:**
    ```bash
    python app.py
    ```

5.  O terminal exibir√° uma mensagem indicando que o servidor foi iniciado. Anote o endere√ßo IP do seu Raspberry Pi na rede.

### 2. Configura√ß√£o do Computador Host (Detector)

**Passos:**

1.  **Clone o reposit√≥rio** no seu computador.

2. **Crie o ambiente na pasta do seu projeto**
    python -m venv venv

    # Ative o ambiente (Windows)
    .\venv\Scripts\activate

    # Ative o ambiente (Linux/macOS)
    source venv/bin/activate

3.  **Instale as depend√™ncias:**
    *   √â altamente recomendado criar um ambiente virtual (`venv`).
    *   Instale o PyTorch seguindo as instru√ß√µes oficiais para seu sistema (CPU ou GPU): [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    
    * pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129

    *   Instale as demais depend√™ncias:
        ```bash
        pip install -r requirements.txt
        ```

4.  **Atualize o endere√ßo do Stream:**
    *   Abra o arquivo `run_detection.py`.
    *   Na linha `STREAM_URL = 'http://192.168.0.131:5000/video_feed'`, **substitua `192.168.0.131` pelo endere√ßo IP real do seu Raspberry Pi**.

5.  **Execute o script de detec√ß√£o:**
    ```bash
    python run_detection.py
    ```

6.  Uma janela do OpenCV aparecer√°, exibindo o v√≠deo ao vivo do Raspberry Pi com os objetos detectados.

## Justificativa das Escolhas

*   **Hardware (Raspberry Pi 3):** A escolha do Raspberry Pi 3 em vez do ESP32-CAM se deu pela maior flexibilidade e poder de processamento. O Raspberry Pi possui um sistema operacional completo (Raspberry Pi OS), facilitando o desenvolvimento com bibliotecas Python robustas como `picamera2` e `Flask`. Sua capacidade de 1GB de RAM √© mais do que suficiente para a tarefa de captura e streaming, garantindo uma transmiss√£o est√°vel e com menor lat√™ncia.

*   **Comunica√ß√£o (Wi-Fi e Flask):** A comunica√ß√£o Wi-Fi √© essencial para a mobilidade do sistema de c√¢mera. O uso de um servidor web com Flask para criar um stream MJPEG √© uma abordagem padr√£o da ind√∫stria, de f√°cil implementa√ß√£o e amplamente compat√≠vel. Qualquer cliente HTTP, como o OpenCV neste projeto, pode consumir o stream sem a necessidade de protocolos complexos.

*   **Software (Python, OpenCV, YOLOv5):** Python foi escolhido por ser a linguagem padr√£o para projetos de IA e Vis√£o Computacional, com um vasto ecossistema de bibliotecas. O OpenCV √© a ferramenta ideal para manipula√ß√£o de imagens e v√≠deo. O YOLOv5 foi utilizado para manter a consist√™ncia com o ambiente do projeto original, aproveitando o modelo `best.pt` j√° treinado para realizar a detec√ß√£o de objetos de forma eficiente no computador host.

## üîß Como executar o c√≥digo

Este projeto foi desenvolvido em **Python** e utiliza **Jupyter Notebook** para documentar todo o fluxo de an√°lise de dados e Machine Learning.

### Pr√©-requisitos

- Python 3.9 ou superior  
- Jupyter Notebook ou Jupyter Lab  
- Bibliotecas Python:
  - `pandas`
  - `numpy`
  - `matplotlib`
  - `seaborn`
  - `scikit-learn`(from `sklearn.model_selection` import `train_test_split`)

> **Dica:** √â recomendado criar um ambiente virtual antes de instalar as bibliotecas.

### Passo a passo

1. **Clonar o reposit√≥rio**  
   ```bash
   git clone https://github.com/danivrf/challenge-fase5-FIAP.git
2. **Navegar at√© a pasta do projeto**
    ```bash
   cd challenge-fase5-FIAP
3. **Instalar as bibliotecas necess√°rias**
   ```bash
   pip install pandas numpy matplotlib seaborn scikit-learn
4. **Abrir o notebook**
   - Entre na pasta `_notebooks`
   - Abra o arquivo `.ipynb` no Jupyter Notebook ou Jupyter Lab
 5. **Executar o notebook**
    - Execute as c√©lulas na ordem, que incluem:
        - An√°lise explorat√≥ria dos dados
        - Prepara√ß√£o do dataset
        - Constru√ß√£o e treinamento do modelo de Machine Learning
        - Visualiza√ß√µes e gr√°ficos de resultados
 **Visualizar resultados**
    - Os gr√°ficos e outputs do notebook mostram insights sobre os dados coletados pelos sensores e as predi√ß√µes do modelo.
   
**Concluindo a Aplica√ß√£o em Tempo Real:** O modelo treinado foi integrado com sucesso em um sistema de edge computing. O sistema se comportou de forma robusta: o Raspberry Pi provou ser uma solu√ß√£o de baixo custo e eficiente para a captura e streaming de v√≠deo com baixa lat√™ncia, enquanto o computador host conseguiu processar o feed em tempo real e aplicar o modelo para detec√ß√£o ao vivo. A comunica√ß√£o entre os dispositivos foi est√°vel e consistente.

## üìã Licen√ßa

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a property="dct:title" rel="cc:attributionURL" href="https://github.com/agodoi/template">MODELO GIT FIAP</a> por <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://fiap.com.br">Fiap</a> est√° licenciado sobre <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">Attribution 4.0 International</a>.</p>

